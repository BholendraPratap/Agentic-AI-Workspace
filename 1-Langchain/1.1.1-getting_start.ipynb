{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING THE ENVIRONMENT OF API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code essentially checks if the GROQ_API_KEY environment variable is already set. If it is, it keeps the existing value. If it's not set, it assigns None to the environment variable.\n",
    "\n",
    "### Purpose:\n",
    "\n",
    "Accessing API Keys: This pattern is often used to securely manage API keys within an application's environment.\n",
    "\n",
    "Instead of hardcoding the API key directly in the code (which is a security risk), it's stored as an environment variable.\n",
    "This allows you to easily change the API key without modifying the code itself.\n",
    "Environment variables are typically not included in version control systems, adding another layer of security.\n",
    "Centralized Configuration: Storing sensitive information like API keys in environment variables helps keep your application's configuration centralized and easier to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000024D1D766000>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024D1D767110>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOADING THE MODEL\n",
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.invoke(\"What is Agentic AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Agentic AI refers to a type of artificial intelligence that is **goal-oriented and autonomous**. \\n\\nThink of it like this:\\n\\n* **Traditional AI:** Often works within predefined rules and parameters, reacting to specific inputs. It's like a calculator; you give it numbers, and it gives you a result. \\n* **Agentic AI:** Has its own goals and objectives, and it takes actions in the world to achieve them. It's like a self-driving car; it has a destination in mind and navigates the roads to get there, making decisions along the way.\\n\\nHere are some key characteristics of Agentic AI:\\n\\n* **Goal-Oriented:** Agentic AI systems are designed with specific goals in mind. They actively seek to achieve these goals through their actions.\\n* **Autonomous:** They can operate independently, making decisions and taking actions without constant human intervention.\\n* **Adaptive:** Agentic AI can learn and adapt to changing environments and circumstances. \\n* **Reactive:** They can respond to events and stimuli in their environment in a meaningful way.\\n* **Proactive:** Agentic AI can take initiative and anticipate future events, planning actions in advance.\\n\\n**Examples of Agentic AI:**\\n\\n* **Self-driving cars:** Navigating roads, avoiding obstacles, and reaching their destination.\\n* **Robotics:** Performing tasks in factories, warehouses, or even homes.\\n* **Game AI:** Controlling non-player characters (NPCs) in video games, making them appear more intelligent and realistic.\\n* **Virtual assistants:** Scheduling appointments, managing emails, and answering questions.\\n\\n**Challenges of Agentic AI:**\\n\\n* **Safety and ethics:** Ensuring that agentic AI systems operate safely and responsibly, avoiding unintended consequences.\\n* **Explainability:** Understanding how agentic AI systems make decisions, which can be complex and opaque.\\n* **Control and oversight:** Establishing mechanisms to monitor and control the actions of agentic AI systems, especially in critical domains.\\n\\nAgentic AI is a rapidly developing field with the potential to revolutionize many aspects of our lives. However, it's crucial to address the ethical and societal challenges it presents to ensure its responsible development and deployment.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 456, 'prompt_tokens': 15, 'total_tokens': 471, 'completion_time': 0.829090909, 'prompt_time': 7.704e-05, 'queue_time': 0.022585828, 'total_time': 0.829167949}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-88f47409-d7f5-4652-aec9-de3286141053-0', usage_metadata={'input_tokens': 15, 'output_tokens': 456, 'total_tokens': 471})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic AI refers to a type of artificial intelligence that is **goal-oriented and autonomous**. \n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "* **Traditional AI:** Often works within predefined rules and parameters, reacting to specific inputs. It's like a calculator; you give it numbers, and it gives you a result. \n",
      "* **Agentic AI:** Has its own goals and objectives, and it takes actions in the world to achieve them. It's like a self-driving car; it has a destination in mind and navigates the roads to get there, making decisions along the way.\n",
      "\n",
      "Here are some key characteristics of Agentic AI:\n",
      "\n",
      "* **Goal-Oriented:** Agentic AI systems are designed with specific goals in mind. They actively seek to achieve these goals through their actions.\n",
      "* **Autonomous:** They can operate independently, making decisions and taking actions without constant human intervention.\n",
      "* **Adaptive:** Agentic AI can learn and adapt to changing environments and circumstances. \n",
      "* **Reactive:** They can respond to events and stimuli in their environment in a meaningful way.\n",
      "* **Proactive:** Agentic AI can take initiative and anticipate future events, planning actions in advance.\n",
      "\n",
      "**Examples of Agentic AI:**\n",
      "\n",
      "* **Self-driving cars:** Navigating roads, avoiding obstacles, and reaching their destination.\n",
      "* **Robotics:** Performing tasks in factories, warehouses, or even homes.\n",
      "* **Game AI:** Controlling non-player characters (NPCs) in video games, making them appear more intelligent and realistic.\n",
      "* **Virtual assistants:** Scheduling appointments, managing emails, and answering questions.\n",
      "\n",
      "**Challenges of Agentic AI:**\n",
      "\n",
      "* **Safety and ethics:** Ensuring that agentic AI systems operate safely and responsibly, avoiding unintended consequences.\n",
      "* **Explainability:** Understanding how agentic AI systems make decisions, which can be complex and opaque.\n",
      "* **Control and oversight:** Establishing mechanisms to monitor and control the actions of agentic AI systems, especially in critical domains.\n",
      "\n",
      "Agentic AI is a rapidly developing field with the potential to revolutionize many aspects of our lives. However, it's crucial to address the ethical and societal challenges it presents to ensure its responsible development and deployment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting up prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here System: this is instruction given to LLM before asking the question.\n",
    "And User key : this is the question asked from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatGroq(model=\"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000024D2324AEA0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024D23248770>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke({'input':'Explain me about how we can used AI for healthcare usecases'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  AI in Healthcare: Revolutionizing Patient Care and Beyond\n",
      "\n",
      "As an AI engineer, I'm excited about the transformative potential of AI in healthcare. It's not just hype; AI is already making a real difference in various use cases, improving patient outcomes, streamlining operations, and empowering healthcare professionals. \n",
      "\n",
      "Here's a glimpse into how AI is being used in healthcare:\n",
      "\n",
      "**1. Diagnostics & Disease Prediction:**\n",
      "\n",
      "* **Image Analysis:** AI excels at analyzing medical images like X-rays, CT scans, and MRIs. It can detect abnormalities like tumors, fractures, and other conditions with accuracy comparable to, or even exceeding, human experts. This speeds up diagnosis and allows for early intervention.\n",
      "* **Predictive Analytics:** By analyzing patient data like medical history, lifestyle factors, and genetic information, AI algorithms can predict the likelihood of developing certain diseases. This enables proactive preventive measures and personalized healthcare strategies.\n",
      "\n",
      "**2. Treatment & Personalized Medicine:**\n",
      "\n",
      "* **Drug Discovery & Development:** AI accelerates the process of discovering and developing new drugs by analyzing vast datasets of molecular structures and clinical trial data. This leads to faster and more efficient drug development cycles.\n",
      "* **Personalized Treatment Plans:** AI can analyze individual patient data to recommend personalized treatment plans tailored to their specific needs, genetic makeup, and medical history.\n",
      "\n",
      "**3. Administrative Efficiency & Patient Experience:**\n",
      "\n",
      "* **Automated Tasks:** AI can automate repetitive administrative tasks like appointment scheduling, billing, and insurance claims processing, freeing up healthcare professionals to focus on patient care.\n",
      "* **Virtual Assistants:** AI-powered chatbots can answer patient queries, provide basic medical information, and schedule appointments, improving patient access to care and reducing wait times.\n",
      "\n",
      "**4. Remote Patient Monitoring & Telemedicine:**\n",
      "\n",
      "* **Wearable Devices:** AI analyzes data from wearable devices like smartwatches and fitness trackers to monitor patient health remotely, detecting potential issues and alerting healthcare providers.\n",
      "* **Teleconsultations:** AI-powered tools enhance telemedicine by providing real-time language translation, symptom checking, and virtual consultations with specialists, expanding access to healthcare in remote areas.\n",
      "\n",
      "**Challenges & Ethical Considerations:**\n",
      "\n",
      "While AI offers immense potential, it's crucial to address challenges and ethical considerations:\n",
      "\n",
      "* **Data Privacy & Security:** Protecting patient data is paramount. Robust security measures and ethical data governance frameworks are essential.\n",
      "* **Bias & Fairness:** AI algorithms can inherit biases from the data they are trained on, leading to disparities in healthcare access and treatment. Addressing biases in training data and developing fair algorithms is crucial.\n",
      "* **Transparency & Explainability:** AI models can be complex and opaque. Ensuring transparency in how AI makes decisions and providing explainable AI is essential for building trust and accountability.\n",
      "\n",
      "**The Future of AI in Healthcare:**\n",
      "\n",
      "AI is poised to revolutionize healthcare, leading to more accurate diagnoses, personalized treatments, improved patient outcomes, and a more efficient healthcare system. \n",
      "\n",
      "As an AI engineer, I believe that by addressing the ethical challenges and fostering collaboration between AI experts, clinicians, and patients, we can unlock the full potential of AI to transform healthcare for the better.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.content)\n",
    "#using this line of code we can view only the content returned by the invoke()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this content or returned values will be different for each type of model we can also restructured the way of displaying the content.\n",
    "\n",
    "### For this we use different output parsers and this is included in chain which we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer, I'm familiar with Langsmith! \n",
      "\n",
      "**Langsmith** is an open-source framework developed by __**Stability AI**__ that aims to simplify the process of developing and deploying **AI assistants**. It's designed to be highly customizable and accessible, allowing users to fine-tune existing models or train their own from scratch. \n",
      "\n",
      "Here are some key features of Langsmith:\n",
      "\n",
      "* **Lightweight and Modular:** Langsmith is built with a modular architecture, making it easy to integrate different components and customize the workflow.\n",
      "* **User-Friendly Interface:** It offers a web-based interface that simplifies model training, evaluation, and deployment.\n",
      "* **Fine-tuning Capabilities:**  Langsmith allows you to fine-tune pre-trained language models (LLMs) on your own datasets, enabling you to create specialized AI assistants for specific tasks or domains.\n",
      "* **Chain-of-Thought Prompting:**  It supports chain-of-thought prompting, a technique that encourages models to reason step-by-step, leading to more coherent and logical responses.\n",
      "* **Evaluation and Monitoring:** Langsmith provides tools for evaluating model performance and monitoring its behavior over time.\n",
      "* **Community-Driven:** Being open-source, Langsmith benefits from a vibrant community of developers who contribute to its development and share resources.\n",
      "\n",
      "**Who can benefit from Langsmith?**\n",
      "\n",
      "* **Researchers:**  Langsmith provides a platform for experimenting with new AI techniques and evaluating their effectiveness.\n",
      "* **Developers:** It simplifies the process of building and deploying custom AI assistants, accelerating development cycles.\n",
      "* **Educators:**  Langsmith can be used as a teaching tool to introduce students to the fundamentals of AI and natural language processing.\n",
      "* **Anyone interested in exploring the potential of AI:** Langsmith's user-friendly interface makes it accessible to individuals with varying levels of technical expertise.\n",
      "\n",
      "**Overall, Langsmith is a powerful and versatile framework that empowers users to harness the capabilities of AI for a wide range of applications.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now this another way of applying parsers here we enclosed parser within the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI Engineer, I can definitely tell you about Langsmith!\n",
      "\n",
      "Langsmith is an open-source framework developed by the MosaicML team designed to simplify the process of fine-tuning large language models (LLMs).  \n",
      "\n",
      "Here's a breakdown of what makes Langsmith special:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **User-Friendly Interface:** It provides a streamlined, web-based interface that makes fine-tuning LLMs accessible even to users without extensive technical expertise. \n",
      "* **Efficient Fine-Tuning:** Langsmith leverages MosaicML's expertise in efficient model training to optimize the fine-tuning process, reducing time and computational resources.\n",
      "* **Automatic Data Preprocessing:** It automates the often tedious task of data preprocessing, saving you time and effort.\n",
      "* **Model Evaluation and Monitoring:** Langsmith includes tools for evaluating the performance of your fine-tuned models and monitoring their progress during training.\n",
      "* **Open-Source and Customizable:** Being open-source, Langsmith allows for community contributions and customization to suit specific needs.\n",
      "\n",
      "**Benefits of Using Langsmith:**\n",
      "\n",
      "* **Accessibility:**  Simplifies fine-tuning for a wider range of users, democratizing access to powerful LLMs.\n",
      "* **Speed and Efficiency:** Reduces the time and resources required for fine-tuning, making it more practical for experimentation and deployment.\n",
      "* **Improved Performance:**  Efficient training methods can lead to better-performing fine-tuned models.\n",
      "* **Transparency and Control:**  Open-source nature provides transparency in the process and allows for greater control over model customization.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "Langsmith is suitable for a variety of applications, including:\n",
      "\n",
      "* **Chatbots and Conversational AI:** Fine-tuning LLMs for specific domains or conversational styles.\n",
      "* **Text Summarization and Generation:**  Tailoring LLMs for generating concise summaries or creative text formats.\n",
      "* **Code Generation:**  Training LLMs to generate code in specific programming languages.\n",
      "* **Language Translation:**  Improving the accuracy of machine translation for specific language pairs.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "If you're interested in exploring Langsmith, you can find detailed documentation and resources on the official MosaicML website and GitHub repository.\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about Langsmith or need further clarification!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### difference between chatprompttemplate and prompt template:\n",
    "PromptTemplate comes from the langchain_core.prompts class. There are effectively two ways to create a prompt.\n",
    "\n",
    "Instantiate using an initializer\n",
    "Here we provide name/names of input vairables in an array and a template that carries a sentence with the input_vairable/variables. This is the basic structure\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"sport\"],\n",
    "    template=\"I love playing {sport}\"\n",
    ")\n",
    "\n",
    "2. Initialize using from_template\n",
    "\n",
    "Usually people initialize Prompts using from_template\n",
    "\n",
    "prompt3 = PromptTemplate.from_template(\"I love visiting {country} because of it's {adjective}\")\n",
    "\n",
    "### Next we will move onto ChatPromptTemplate:\n",
    "\n",
    "This is useful when we are creating ChatBot or Question-Answering system. This comes from the langchain_core.prompts class. We will firstly initialize a ChatOpenAI model that will use ChatPromptTemplate to interact with OpenAI.\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI()\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "ChatPromptTemplate has three important messagetype:\n",
    "\n",
    "System Message: This is used to baseline the AI assistant. If you want to set some behaviour of your assistant. This is the place to do it.\n",
    "2. HumanMessage: All the user inputs are to stored in this\n",
    "\n",
    "3. AIMessage: This stores the response from the LLM.\n",
    "\n",
    "Let’s look at the ways we can initialize ChatPromptTemplate:\n",
    "\n",
    "Basic way: In this we create an array of tuples. System, human and AI messages are stored in tuples using key name as “system”, ”human”, “ai”\n",
    "chat = ChatPromptTemplate.from_messages([\n",
    "   (\"system\",\"You are a helpful AI Assistant with a sense of humor\"),\n",
    "   (\"human\",\"Hi how are you?\"),\n",
    "   (\"ai\",\"I am good. How can I help you?\"),\n",
    "   (\"human\",\"{input}\")\n",
    "])\n",
    "\n",
    "\n",
    "Once the ChatPromptTemplate is created, we can populate the input using format_messages and put this inside the model to fetch answer from OpenAI LLM.\n",
    "\n",
    "chat1= chat.format_messages(input=\"What is the capital of South Africa?\")\n",
    "model.invoke(chat1)\n",
    "\n",
    "\n",
    "As you can see, we gave our ChatPromptTemplate a personality using “system” message that it should respond with a humorous touch. It is reflected in the answer as well.\n",
    "\n",
    "2. Using respective PromptTemplate\n",
    "\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "chatprompt2 = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful assistant with a {personality} personality\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG :building a basic RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=WebBaseLoader(\"https://python.langchain.com/docs/tutorials/llm_chain/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x24d22668350>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt templatesOn this pageBuild a simple LLM application with chat models and prompt templates\\nIn this quickstart we\\'ll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it\\'s just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you\\'ll have a high level overview of:\\n\\n\\nUsing language models\\n\\n\\nUsing prompt templates\\n\\n\\nDebugging and tracing your application using LangSmith\\n\\n\\nLet\\'s dive in!\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.\\n\\nSelect chat model:Groq▾GroqOpenAIAnthropicAzureGoogle VertexAWSCohereNVIDIAFireworks AIMistral AITogether AIDatabrickspip install -qU \"langchain[groq]\"import getpassimport osif not os.environ.get(\"GROQ_API_KEY\"):  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(\"Translate the following from English into Italian\"),    HumanMessage(\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage\\nAIMessage(content=\\'Ciao!\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 3, \\'prompt_tokens\\': 20, \\'total_tokens\\': 23, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-32654a56-627c-40e1-a141-ad9350bbfd3e-0\\', usage_metadata={\\'input_tokens\\': 20, \\'output_tokens\\': 3, \\'total_tokens\\': 23, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\ntipIf we\\'ve enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\\nNote that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:\\n\\nlanguage: The language to translate text into\\ntext: The text to translate\\n\\nfrom langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate\\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\\nChatPromptValue(messages=[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})])\\nWe can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion\\u200b\\nThat\\'s it! In this tutorial you\\'ve learned how to create your first simple LLM application. You\\'ve learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we\\'ve got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we\\'ve got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\\n\\nChat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pageWas this page helpful?PreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document=loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt templatesOn this pageBuild a simple LLM application with chat models and prompt templates'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you'll have a high level overview of:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"Using language models\\n\\n\\nUsing prompt templates\\n\\n\\nDebugging and tracing your application using LangSmith\\n\\n\\nLet's dive in!\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='PipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Or, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Select chat model:Groq▾GroqOpenAIAnthropicAzureGoogle VertexAWSCohereNVIDIAFireworks AIMistral AITogether AIDatabrickspip install -qU \"langchain[groq]\"import getpassimport osif not os.environ.get(\"GROQ_API_KEY\"):  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(\"Translate the following from English into Italian\"),    HumanMessage(\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\\ntipIf we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='for token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='language: The language to translate text into\\ntext: The text to translate'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='from langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate\\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\\nChatPromptValue(messages=[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})])'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"We can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion\\u200b\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"Conclusion\\u200b\\nThat's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we've got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Chat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pageWas this page helpful?PreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(document)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading hugging face api environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  #load all the environment variables\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\AgenticAIWorkspace\\venv1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "j:\\AgenticAIWorkspace\\venv1\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BHOLENDRA PRATAP\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04311221092939377, 0.13562119007110596, 0.022339938208460808, 0.00721672922372818, 0.03421058505773544, 0.024034056812524796, -0.024848803877830505, 0.04566733539104462, 0.01885056309401989, 0.04899343103170395, -0.004306023009121418, 0.05968935415148735, 0.002952256705611944, -0.05999084189534187, -0.11980380117893219, -0.005690651945769787, -0.02096848003566265, 0.009721233509480953, 0.04023448005318642, 0.05047000199556351, -0.0021607629023492336, 0.09888076782226562, 0.021964700892567635, -0.058519911020994186, 0.029561879113316536, 0.00411768676713109, -0.09333004057407379, -0.04305519163608551, 0.0696839839220047, -0.04684080183506012, 0.04395323991775513, 0.010073358193039894, 0.09620824456214905, 0.027930229902267456, 0.07333722710609436, -0.012976894155144691, 0.0761367455124855, -0.011923196725547314, 0.011215245351195335, -0.008163190446794033, -0.010897384956479073, -0.07058070600032806, -0.027595993131399155, -0.006153077818453312, 0.04643205553293228, 0.05815570801496506, -0.037044983357191086, -0.017532601952552795, -0.045897096395492554, 0.05265127867460251, -0.1262216717004776, -0.064104363322258, -0.0004426657105796039, -0.0034306379966437817, 0.04251396283507347, 0.04050421342253685, 0.01263436023145914, 0.017847387120127678, -0.08135198056697845, -0.045755721628665924, 0.011493436992168427, 0.024737831205129623, -0.1136598289012909, 0.02093520760536194, 0.08893708139657974, 0.0715235248208046, 0.016180438920855522, 0.007937218062579632, -0.004844050854444504, -0.07771333307027817, -0.024759287014603615, -0.006974477786570787, -0.009442347101867199, 0.025737067684531212, -0.028032012283802032, -0.00758296949788928, 0.001208719564601779, 0.021890800446271896, -0.01422956958413124, -0.040772128850221634, -0.031037040054798126, -0.011769535951316357, -0.017414219677448273, -0.005468793213367462, -0.021282609552145004, 0.003637513145804405, -0.028120877221226692, -0.07080921530723572, -0.019715819507837296, 0.014832294546067715, 0.02632725238800049, -0.10834402590990067, 0.059244826436042786, 0.006977147422730923, -0.03420635312795639, -0.08328738808631897, 0.01941866986453533, 0.03152834624052048, 0.09671329706907272, 0.08736559003591537, 0.00045874775969423354, 0.055626124143600464, 0.028303470462560654, -0.007492844946682453, -0.08239169418811798, -0.05510362610220909, -0.03598214313387871, -0.0661410242319107, 0.023383567109704018, -0.01034215185791254, -0.025135580450296402, -0.04364794120192528, -0.0930371806025505, -0.08427294343709946, -0.005717211402952671, -0.047260135412216187, -0.054062094539403915, -0.00630444148555398, 0.06324265152215958, 0.006845063529908657, 0.007304769475013018, -0.03741742670536041, -0.06619557738304138, -0.03141272813081741, -0.04530555009841919, -0.13435551524162292, 0.06425638496875763, -3.7944564588619885e-33, 0.009204626083374023, -0.002612070180475712, -0.05146433040499687, 0.02832675725221634, 0.01586945354938507, -0.018390115350484848, 0.009616615250706673, -0.059003304690122604, -0.07848989218473434, -0.058791954070329666, -0.05810655280947685, -0.0037340200506150723, 0.01943146623671055, 0.03566228970885277, -0.012873148545622826, 0.017555130645632744, -0.05345636233687401, 0.10923226922750473, 0.053074128925800323, 0.018666181713342667, 0.011073928326368332, 0.01862945780158043, 0.01849416084587574, -0.03328321501612663, 0.08108533173799515, 0.019820045679807663, 0.003603970166295767, -0.05225149169564247, -0.016028685495257378, 0.03214481845498085, 0.07309551537036896, -0.010538838803768158, -0.014300440438091755, -0.07095123827457428, 0.015454120934009552, 0.045815352350473404, 0.013757501728832722, -0.05760258436203003, 0.02660336159169674, -0.010926456190645695, 0.03702102601528168, -0.03239743784070015, 0.10511930286884308, 0.013426808640360832, 0.03451130911707878, 0.08210955560207367, 0.013097532093524933, 0.028171364217996597, 0.17125359177589417, 0.02431269735097885, -0.0045232633128762245, -0.029154542833566666, -0.07677106559276581, -0.046114105731248856, 0.04300985112786293, 0.016595100983977318, -0.03037750907242298, 0.06007708981633186, 0.008674858137965202, 0.047079794108867645, -0.006106652319431305, 0.045484766364097595, -0.013986222445964813, -0.018740186467766762, -0.05967800319194794, -0.05526150390505791, -0.03951441869139671, -0.02635776251554489, 0.06813975423574448, -0.050848379731178284, -0.11309292912483215, 0.03381049633026123, 0.03922306001186371, 0.0016803781036287546, -0.0394345261156559, 0.047680530697107315, 0.026041721925139427, -0.011414683423936367, -0.05137057229876518, 0.013942430727183819, -0.14343897998332977, -0.026646550744771957, 0.04094994068145752, 0.05247935652732849, 0.01476326771080494, 0.04357632249593735, 0.02719494141638279, -0.00493119889870286, 0.019762380048632622, 0.05359716713428497, -0.0354665070772171, -0.03361392766237259, -0.08613725751638412, -0.007034134119749069, 0.02001785673201084, 1.9721143827470743e-33, -0.040057338774204254, -0.08697954565286636, -0.10752365738153458, 0.04659420624375343, -0.021553553640842438, 0.06345474720001221, -0.0012436695396900177, 0.1024932861328125, -0.03840319439768791, 0.05636506900191307, 0.03455967828631401, -0.010708617977797985, 0.09041696041822433, -0.003870359854772687, 0.007843250408768654, -0.006417830474674702, 0.07932409644126892, -0.03681078180670738, -0.034556373953819275, 0.016351623460650444, -0.058682091534137726, 0.061020705848932266, 0.03319576010107994, 0.035559166222810745, 0.04192164167761803, 0.02741735242307186, -0.0012143185595050454, -0.08340507745742798, -0.027781490236520767, -0.0020404253154993057, 0.05238412320613861, -0.011442574672400951, -0.15991191565990448, 0.10775578022003174, -0.05469134822487831, -0.05283842980861664, 0.07845483720302582, -0.0084429532289505, -0.03675151243805885, 0.0917014628648758, 0.06818995624780655, 0.06533981114625931, -0.02763957530260086, 0.022009488195180893, -0.023787004873156548, -0.052461352199316025, -0.02089497074484825, 0.037815697491168976, 0.052156537771224976, 0.0008395061013288796, 0.08176426589488983, -0.008409171365201473, 0.06646206974983215, -0.03922244533896446, 0.006130001973360777, 0.0671042650938034, -0.03348521143198013, -0.07163231819868088, -0.01698315143585205, -0.007842091843485832, 0.07378991693258286, 0.06141798943281174, -0.0687808096408844, 0.06929198652505875, 0.03963562101125717, -0.07560233771800995, -0.07143716514110565, 0.030205760151147842, -0.06733661144971848, 0.011503313668072224, 0.04433317109942436, -0.09573148936033249, -0.06681518256664276, -0.059916768223047256, 0.040237948298454285, -0.023708675056695938, 0.02965998463332653, 0.03539906442165375, -0.020465901121497154, -0.029972653836011887, 0.09210741519927979, 0.04007172957062721, -0.024441014975309372, 0.05752194672822952, 0.03931914642453194, 0.015297857113182545, -0.04066697135567665, -0.09090721607208252, 0.038153503090143204, 0.018748730421066284, -0.09823141247034073, -0.0007511866861023009, 0.029044361785054207, 0.050311218947172165, -0.04463543742895126, -1.5695729160825067e-08, -0.026139548048377037, -0.03394683450460434, -0.036586467176675797, -0.014450575225055218, -0.06972284615039825, -0.030430257320404053, 0.02459660917520523, -0.0026659907307475805, -0.06651010364294052, 0.028640104457736015, 0.02950911782681942, 0.000487333454657346, -0.08660135418176651, -0.07713273912668228, 0.0032889125868678093, -0.019061801955103874, 0.06681746244430542, -0.03688705340027809, -0.06013091653585434, 0.012280246242880821, 0.029498452320694923, 0.02757936157286167, 0.033414509147405624, -0.05373019725084305, 0.011133738793432713, 0.0581592433154583, 0.05232199281454086, 0.02633587270975113, 0.09067613631486893, -0.007127203047275543, -0.02501193806529045, 0.09089883416891098, 0.02069024369120598, 0.004512567538768053, 0.018687164410948753, 0.05480154603719711, 0.08980017900466919, 0.031787678599357605, -0.019696030765771866, 0.00925370678305626, 0.006232331041246653, -0.04840949550271034, -0.00243024667724967, 0.06945980340242386, 0.022277707234025, 0.008216606453061104, -0.0450674444437027, -0.08695458620786667, 0.0018756353529170156, -0.08053699880838394, -0.0695214569568634, -0.03903593495488167, 0.08881518244743347, 0.09672386944293976, -0.06779690086841583, 0.013228235766291618, 0.0478711873292923, 0.02026943303644657, 0.020969219505786896, -0.015539176762104034, 0.07723181694746017, 0.05705316364765167, 0.07158512622117996, 0.07126480340957642]\n",
      "\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "#checking that embedding is performin gor not.\n",
    "text=\"this is atest documents\"\n",
    "query_result=embeddings.embed_query(text)\n",
    "print(query_result)\n",
    "print()\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x24d508af200>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting the whole documents chunks into vectors and storing them into vector stores.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore=FAISS.from_documents(documents,embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now vectors are stored now searching for query in vector\n",
    "query=\"This is a relatively simple LLM application \"\n",
    "\n",
    "result=vectorstore.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now till here we have created basic RAG , now we will integrate it with llm : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Here integrating is basically mean that we are going to pass a augmented prompt to llm for giving the output of user query.\n",
    "2.Now to do so we will use ChatPromptTemplate this is because of the following reasons:\n",
    "-- in this we have option to give question with a context that is not in simple prompt template.\n",
    "-- and whenever we deal with models chatopenai and others, we have to use so that a runnable binding can create which gives option to give context that is not in prompttemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "context: here this is contextual output which will comes after querying vector database.And this is basically the important information required by the llm to give a precised output of a user query.\n",
    "\n",
    "NOw we can see that obviously this contextual output will not of one line it will be big so we have need to chain them.\n",
    "\n",
    "to do show we use create stuff documnets chain :https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html#langchain.chains.combine_documents.stuff.create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example code only :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"What are everyone's favorite colors:\\n\\n{context}\"), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000024D53A74770>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024D53A75490>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install -U langchain langchain-community\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"What are everyone's favorite colors:\\n\\n{context}\")] \n",
    ")\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\")\n",
    "chain = create_stuff_documents_chain(model, prompt) #this is another of creating chain as we done previously.\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see that a runnable binding is created using create_stuff_documnets which has all components ChatpromptTemplate,ChatGroq,StrOutputParser as like we created chain previously.\n",
    "But most importantly that this runnable binding gives you option to invoke the whole chain with a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's what we know about everyone's favorite colors:\\n\\n* **Jesse:** Loves red, dislikes yellow\\n* **Jamal:** Loves green, loves orange more than green \\n\\nWe don't know about anyone else's favorite colors!  \\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#giving context externally as our own not from vector store.\n",
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content = \"Jamal loves green but not as much as he loves orange\")\n",
    "]\n",
    "\n",
    "chain.invoke({\"context\": docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here our followed code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"What are everyone's favorite colors:\\n\\n{context}\"), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000024D53A74770>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024D53A75490>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(model,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to get contextual output for our query actually we asks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1117fb56-a741-4d3e-b20a-d7c15d90376b', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b'),\n",
       " Document(id='94d74c8c-7cc2-4c58-b859-8341431f70bf', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"We can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion\\u200b\"),\n",
       " Document(id='ca068cc0-fa2a-4911-a847-9c483e64cd74', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Chat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pageWas this page helpful?PreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.'),\n",
       " Document(id='a965824d-9cc1-432a-b452-0cc2a0d3958c', metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | 🦜️🔗 LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='for token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"Note that ChatModels receive message objects as input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that contextual output is within page content attribute of above output.\n",
    "And below invoking it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I don't have personal preferences like favorite colors.  \\n\\nI can tell you that color preferences are very subjective and vary widely from person to person! Some popular colors include:\\n\\n* **Blue:** Often associated with calmness, trust, and stability.\\n* **Green:** Represents nature, growth, and harmony.\\n* **Red:**  Signifies energy, passion, and excitement.\\n* **Purple:**  Linked to royalty, creativity, and mystery.\\n* **Yellow:**  Evokes happiness, optimism, and warmth. \\n\\nWhat's your favorite color, and what do you like about it? \\n\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain.invoke({\n",
    "    \"input\":\"Note that ChatModels receive message objects as input\",\n",
    "    \"context\":[Document(page_content=\"Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke('Hello')model.invoke([{'role': 'user', 'content': 'Hello'}])model.invoke([HumanMessage('Hello')]) StreamingBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end='|')|C|iao|!|| You can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now we have giving our prompt and interacting it wit llm based on context.But you can see that we giving our context that is what we find it out from vector database based on similiarity search .Now question arises that every time we have to do it manually, answer is no : we will make this vector store as interface to ask any question every time and this is one by making it as retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000024D508AF200>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"What are everyone's favorite colors:\\n\\n{context}\"), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000024D53A74770>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024D53A75490>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever() ##retriever is a interface\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)#passing vector store and llm|prompt.\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that we can invoke whole chain and only need to give input question no need to give context manually it is integerated by making runnable binding.\n",
    "\n",
    "in summary:retreiver| create stuff documents chain | prompt | llm | output parser.\n",
    "\n",
    "here retreiver and this chain will dynamic according to our query so that is why we make this as runnable binding so it can integrate with any chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=retrieval_chain.invoke({\"input\":\"Note that ChatModels receive message objects as input\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieval chain: this is used to combine documnet chain and the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It seems like you're asking about favorite colors, but the provided text focuses on LangChain and how to interact with chat models. \\n\\nTo answer your question about favorite colors, I can tell you that people have diverse preferences! \\n\\nSome popular favorite colors include:\\n\\n* **Blue:** Often associated with calmness, trust, and peace.\\n* **Green:** Represents nature, growth, and harmony.\\n* **Red:**  Symbolizes passion, energy, and excitement.\\n* **Purple:**  Linked to royalty, creativity, and mystery.\\n* **Yellow:**  Conveys happiness, optimism, and intellect.\\n\\nHowever, individual preferences vary greatly based on personal experiences, cultural influences, and individual taste. \\n\\n\\nLet me know if you have any other questions about LangChain or anything else! \\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
